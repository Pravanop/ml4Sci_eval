{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task 1 (Common)",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1YLavGPcic2yra9zxsG19-5m8URPwD5eY",
      "authorship_tag": "ABX9TyMd8lIZq/swLzjU4qHGVcwb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pravanop/ml4Sci_eval/blob/main/Task_1_(Common).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fDXoII41Ereh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0177347-b0c9-4c97-b206-04b56f461cbc"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hKQFVXSjEv7s"
      },
      "source": [
        "import h5py\n",
        "import numpy as np\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8MsIPOxGHEYV"
      },
      "source": [
        "## Reading data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vltbFhPWHH6N"
      },
      "source": [
        "### Electron Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kYtPaSzLE3Fv",
        "outputId": "410c9895-343c-44e9-d6a6-bf3e8d41aa0f"
      },
      "source": [
        "data_elec = h5py.File('/content/drive/MyDrive/GSOC/Application/ML4Sci/Task 1/SingleElectronPt50_IMGCROPS_n249k_RHv1.hdf5','r')\n",
        "data_elec.keys()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<KeysViewHDF5 ['X', 'y']>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GrhPrTy9GeYd"
      },
      "source": [
        "electron_X = data_elec['X']\n",
        "electron_Y = data_elec['y']"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8LPTxCRFUtG",
        "outputId": "d42b4fd6-2e89-4f1e-abee-54261091c4d6"
      },
      "source": [
        "#checking shapes\n",
        "print(electron_X.shape)\n",
        "print(electron_Y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(249000, 32, 32, 2)\n",
            "(249000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUPBn7FrHQbe"
      },
      "source": [
        "### Photon Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jt8oaZf0FeHi"
      },
      "source": [
        "data_pho = h5py.File('/content/drive/MyDrive/GSOC/Application/ML4Sci/Task 1/SinglePhotonPt50_IMGCROPS_n249k_RHv1.hdf5','r')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNMkJMhTFuA8"
      },
      "source": [
        "photon_X = data_pho['X']\n",
        "photon_Y = data_pho['y']"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3orpTocFF5cv",
        "outputId": "4d21d026-06f8-40dc-a591-ac1b3ccf6eb5"
      },
      "source": [
        "print(photon_X.shape)\n",
        "print(photon_Y.shape)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(249000, 32, 32, 2)\n",
            "(249000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w0P_eLqPFbmB",
        "outputId": "d40e399e-3f79-40e2-82db-970cac988f6b"
      },
      "source": [
        "#checking assignment of labels\n",
        "print(electron_Y[42])\n",
        "print(photon_Y[42])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jvjrxb2eF-Ye"
      },
      "source": [
        "#joining both datasets\n",
        "X = np.concatenate((np.array(photon_X), np.array(electron_X)))\n",
        "Y = np.concatenate((np.array(photon_Y), np.array(electron_Y)))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNgiSt53HxtA",
        "outputId": "12121e8a-edab-4823-a30c-460af331099e"
      },
      "source": [
        "#checking shapes\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(498000, 32, 32, 2)\n",
            "(498000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "scHa7AktI-Oo"
      },
      "source": [
        "## Building model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlqdJDrbIDtN"
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42) #test split of 0.25"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPZR7qniVDm2",
        "outputId": "512281ba-e26a-4f1d-e51a-c296ddcf2732"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(373500, 32, 32, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUxi0Y6pUtUa"
      },
      "source": [
        "y_train, y_test = y_train.reshape(y_train.shape[0],1), y_test.reshape(y_test.shape[0],1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U62CV6vSJVcS"
      },
      "source": [
        "### Tensorflow version"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfhT5rwbGkc_"
      },
      "source": [
        "### CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZtC3CGdyfldb"
      },
      "source": [
        "from keras.layers import LSTM\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Input, Dense, Flatten, Dropout, Activation, Lambda, Permute, Reshape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q1pP18KlRTGQ"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Flatten, MaxPooling2D, GlobalAveragePooling2D"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WPodYs_oI7Sy"
      },
      "source": [
        "#create model\n",
        "model = Sequential()\n",
        "#add model layers\n",
        "model.add(Conv2D(128, kernel_size= (3,3), activation= 'relu', input_shape=(32,32,2)))\n",
        "model.add(Conv2D(64, kernel_size= (3,3), activation= 'relu'))\n",
        "model.add(MaxPooling2D((2, 2), padding = 'valid'))\n",
        "model.add(Conv2D(32, kernel_size= (3, 3), activation= 'relu'))\n",
        "model.add(Conv2D(32, kernel_size= (3, 3), activation= 'relu'))\n",
        "model.add(Conv2D(32, kernel_size= (3, 3), activation= 'relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(128, activation = 'relu'))\n",
        "model.add(Dense(32, activation = 'relu'))\n",
        "model.add(Dense(1, activation= 'sigmoid'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fN5K2084RvAF",
        "outputId": "04e8d439-77f6-4810-93bd-6557c27af1fd"
      },
      "source": [
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs = 10,batch_size = 256)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1459/1459 [==============================] - 80s 32ms/step - loss: 0.6398 - accuracy: 0.6301 - val_loss: 0.5801 - val_accuracy: 0.7013\n",
            "Epoch 2/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5775 - accuracy: 0.7052 - val_loss: 0.5699 - val_accuracy: 0.7108\n",
            "Epoch 3/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5632 - accuracy: 0.7171 - val_loss: 0.5585 - val_accuracy: 0.7200\n",
            "Epoch 4/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5542 - accuracy: 0.7240 - val_loss: 0.5513 - val_accuracy: 0.7255\n",
            "Epoch 5/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5492 - accuracy: 0.7276 - val_loss: 0.5526 - val_accuracy: 0.7254\n",
            "Epoch 6/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5460 - accuracy: 0.7298 - val_loss: 0.5504 - val_accuracy: 0.7283\n",
            "Epoch 7/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5417 - accuracy: 0.7329 - val_loss: 0.5455 - val_accuracy: 0.7294\n",
            "Epoch 8/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5415 - accuracy: 0.7332 - val_loss: 0.5443 - val_accuracy: 0.7319\n",
            "Epoch 9/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5368 - accuracy: 0.7370 - val_loss: 0.5432 - val_accuracy: 0.7318\n",
            "Epoch 10/10\n",
            "1459/1459 [==============================] - 45s 31ms/step - loss: 0.5345 - accuracy: 0.7374 - val_loss: 0.5467 - val_accuracy: 0.7298\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f45900b37d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF8Nal8mGp8Q"
      },
      "source": [
        "### LSTM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ut9t7KbLgGO8"
      },
      "source": [
        "X_train, X_test = X_train.reshape(X_train.shape[0], 1, X_train.shape[1]*X_train.shape[2]*X_train.shape[3]), X_test.reshape(X_test.shape[0], 1, X_test.shape[1]*X_test.shape[2]*X_test.shape[3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zs1jRFmhUl-E"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "input_shape=(X_train.shape[1],X_train.shape[2])\n",
        "\n",
        "model.add(LSTM(units= 64,return_sequences=True,input_shape=input_shape))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=32,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=16,return_sequences=True))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(LSTM(units=16,input_shape=input_shape)) \n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics = 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "62OzyJUEgmhv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d8e38ae-b03d-4f7a-fe97-87a6a22ab1f8"
      },
      "source": [
        "model.fit(X_train,y_train, validation_data = (X_test, y_test), epochs= 10,batch_size= 128)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "2918/2918 [==============================] - 65s 10ms/step - loss: 0.6642 - accuracy: 0.5947 - val_loss: 0.6383 - val_accuracy: 0.6387\n",
            "Epoch 2/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.6374 - accuracy: 0.6412 - val_loss: 0.6315 - val_accuracy: 0.6471\n",
            "Epoch 3/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.6297 - accuracy: 0.6502 - val_loss: 0.6246 - val_accuracy: 0.6556\n",
            "Epoch 4/10\n",
            "2918/2918 [==============================] - 28s 9ms/step - loss: 0.6195 - accuracy: 0.6625 - val_loss: 0.6173 - val_accuracy: 0.6678\n",
            "Epoch 5/10\n",
            "2918/2918 [==============================] - 28s 10ms/step - loss: 0.6098 - accuracy: 0.6735 - val_loss: 0.6115 - val_accuracy: 0.6731\n",
            "Epoch 6/10\n",
            "2918/2918 [==============================] - 28s 9ms/step - loss: 0.6023 - accuracy: 0.6816 - val_loss: 0.6068 - val_accuracy: 0.6787\n",
            "Epoch 7/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.5925 - accuracy: 0.6931 - val_loss: 0.6019 - val_accuracy: 0.6837\n",
            "Epoch 8/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.5866 - accuracy: 0.6984 - val_loss: 0.6003 - val_accuracy: 0.6887\n",
            "Epoch 9/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.5796 - accuracy: 0.7042 - val_loss: 0.5970 - val_accuracy: 0.6904\n",
            "Epoch 10/10\n",
            "2918/2918 [==============================] - 27s 9ms/step - loss: 0.5732 - accuracy: 0.7114 - val_loss: 0.5965 - val_accuracy: 0.6925\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fdba00c0490>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oQ89X41pGzNj"
      },
      "source": [
        "### VGGNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPu9VFB_hIyU"
      },
      "source": [
        "vgg = tf.keras.applications.VGG16(include_top = False, input_shape = (X_train.shape[1],X_train.shape[2],X_train.shape[3]), classes = 2, classifier_activation= 'sigmoid', weights = None )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVjNWNcSZdbM"
      },
      "source": [
        "global_avg = GlobalAveragePooling2D()\n",
        "pred_layer = Dense(1, activation='sigmoid')\n",
        "net = Sequential([vgg,global_avg,pred_layer])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3CNvmrHakHe"
      },
      "source": [
        "net.compile(optimizer='adam', loss='binary_crossentropy', metrics = 'accuracy')\n",
        "net.fit(X_train,y_train, validation_data = (X_test, y_test), epochs= 10,batch_size= 256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z83fzb7J1JRQ"
      },
      "source": [
        "## Pytorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eufo6vLG6gq"
      },
      "source": [
        "CNN was best so trying that out in pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J9-A92Una1Lq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
        "from torch.optim import Adam, SGD\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xNJY_Nnm6Uaw"
      },
      "source": [
        "from torchvision import datasets, transforms \n",
        "from torch.utils import data  \n",
        "dataset = [X,Y]\n",
        "loader = data.DataLoader(dataset, batch_size = 8, shuffle = True)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hs9N7I_1Qen"
      },
      "source": [
        "class Net(Module):   \n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.cnn_layers = Sequential(\n",
        "            # Defining a 2D convolution layer\n",
        "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(4),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "            # Defining another 2D convolution layer\n",
        "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
        "            BatchNorm2d(4),\n",
        "            ReLU(inplace=True),\n",
        "            MaxPool2d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        self.linear_layers = Sequential(\n",
        "            Linear(4 * 7 * 7, 10)\n",
        "        )\n",
        "\n",
        "    # Defining the forward pass    \n",
        "    def forward(self, x):\n",
        "        x = self.cnn_layers(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear_layers(x)\n",
        "        return x\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pMZb486Bjd5j",
        "outputId": "cce741c2-e5f6-436a-a109-6a30a0efbdc3"
      },
      "source": [
        "model = Net()\n",
        "# defining the optimizer\n",
        "optimizer = Adam(model.parameters(), lr=0.07)\n",
        "# defining the loss function\n",
        "criterion = CrossEntropyLoss()\n",
        "# checking if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "    criterion = criterion.cuda()\n",
        "    \n",
        "print(model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (cnn_layers): Sequential(\n",
            "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (2): ReLU(inplace=True)\n",
            "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (linear_layers): Sequential(\n",
            "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGDDdZ6bnB70"
      },
      "source": [
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=2, gamma=0.1)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e7hkYqemwRx"
      },
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=10):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            current_loss = 0.0\n",
        "            current_corrects = 0\n",
        "\n",
        "            print('Iterating through data...')\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.permute(0, 3, 1, 2)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels.long())\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                current_loss += loss.item() * inputs.size(0)\n",
        "                current_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = current_loss / dataset_sizes[phase]\n",
        "            epoch_acc = current_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # Make a copy of the model if the accuracy on the validation set has improved\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_since = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_since // 60, time_since % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvMHmm86mfCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}